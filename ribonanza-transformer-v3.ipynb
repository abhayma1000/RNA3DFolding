{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "* Take  what I learned in ```ribonanza-3d-finetune-v2.ipynb``` and go further with it\n",
    "* Make some improvements, specially to null handling\n",
    "* Expand the model to do better and use newer technologies like transfomers\n",
    "* Better evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils\n",
    "import utils\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "GPUs = GPUtil.getGPUs()\n",
    "if GPUs:\n",
    "    gpu = GPUs[0]\n",
    "    print(f\"Running on: {gpu.name}\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CONFIG & SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Set a random seed for Python, NumPy, PyTorch (CPU & GPU) to ensure reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Example configuration (you can load this from a YAML, JSON, etc.)\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"cutoff_date\": \"2020-01-01\",\n",
    "    \"test_cutoff_date\": \"2022-05-01\",\n",
    "    \"max_len\": 384,\n",
    "    \"batch_size\": 1,\n",
    "\n",
    "    # change to kaggle\n",
    "    \"model_config_path\": \"ribonanzanet2d-final/configs/pairwise.yaml\",\n",
    "    \n",
    "    \"max_len_filter\": 9999999,\n",
    "    \"min_len_filter\": 10,\n",
    "    \n",
    "    # change to kaggle\n",
    "    \"ribonanzanet2d-final_path\": \"ribonanzanet2d-final\",\n",
    "    \"train_sequences_path\": \"stanford-rna-3d-folding/train_sequences.csv\",\n",
    "    \"train_labels_path\": \"stanford-rna-3d-folding/train_labels.csv\",\n",
    "    \"test_sequences_path\": \"stanford-rna-3d-folding/test_sequences.csv\",\n",
    "    \"pretrained_weights_path\": \"ribonanzanet-weights/RibonanzaNet.pt\",\n",
    "\n",
    "\n",
    "    \"save_weights_folder\": \"trained_model_weights\",\n",
    "    \"save_weights_name\": \"RibonanzaNet-3D.pt\",\n",
    "    \"save_weights_final\": \"RibonanzaNet-3D-final.pt\",\n",
    "}\n",
    "\n",
    "if not os.path.exists(config['save_weights_folder']):\n",
    "    os.mkdir(config['save_weights_folder'])\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA LOADING & PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "train_sequences = pd.read_csv(config[\"train_sequences_path\"])\n",
    "train_labels = pd.read_csv(config[\"train_labels_path\"])\n",
    "\n",
    "test_sequences = pd.read_csv(config[\"test_sequences_path\"])\n",
    "\n",
    "# Create a pdb_id field\n",
    "train_labels[\"pdb_id\"] = train_labels[\"ID\"].apply(\n",
    "    lambda x: x.split(\"_\")[0] + \"_\" + x.split(\"_\")[1]\n",
    ")\n",
    "\n",
    "# Collect xyz data for each sequence\n",
    "# all_xyz = []\n",
    "# for pdb_id in tqdm(train_sequences[\"target_id\"], desc=\"Collecting XYZ data\"):\n",
    "#     df = train_labels[train_labels[\"pdb_id\"] == pdb_id]\n",
    "#     xyz = df[[\"x_1\", \"y_1\", \"z_1\"]].to_numpy().astype(\"float32\")\n",
    "#     xyz[xyz < -1e17] = float(\"nan\")\n",
    "#     all_xyz.append({\"pdb_id\": pdb_id, \"xyz\": xyz})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change to pandas implementation so can keep track of which REsid and which resname with each removed entry\n",
    "\n",
    "nan_ratio_threshold = 0.5\n",
    "max_len_seen = 0\n",
    "num_removed_because_nan = 0\n",
    "num_removed_because_size = 0\n",
    "num_beginning_removed = 0\n",
    "num_ending_removed = 0\n",
    "\n",
    "valid_indices = []\n",
    "\n",
    "print(f\"Number of sequences before filtering: {len(all_xyz)}\")\n",
    "\n",
    "for i, entry in enumerate(all_xyz):\n",
    "    entry_xyz = entry[\"xyz\"]\n",
    "    entry_pdb_id = entry[\"pdb_id\"]\n",
    "\n",
    "    nan_ratio = np.isnan(entry_xyz).mean()\n",
    "\n",
    "    if nan_ratio > nan_ratio_threshold:\n",
    "        num_removed_because_nan += 1\n",
    "        continue\n",
    "    \n",
    "    if len(entry_xyz) > config[\"max_len_filter\"] or len(entry_xyz) < config[\"min_len_filter\"]:\n",
    "        num_removed_because_size += 1\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Remove NaN rows from the beginning until reach a valid row\n",
    "    i = 0\n",
    "    while True:\n",
    "        if np.any(np.isnan(entry_xyz[i, :])):\n",
    "            num_beginning_removed += 1\n",
    "            entry_xyz = np.delete(entry_xyz, i, axis=0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Remove NaN rows from the end until reach a valid row\n",
    "    i = -1\n",
    "    while True:\n",
    "        if np.any(np.isnan(entry_xyz[i, :])):\n",
    "            num_ending_removed += 1\n",
    "            entry_xyz = np.delete(entry_xyz, i, axis=0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    all_xyz[i][\"xyz\"] = entry_xyz\n",
    "\n",
    "\n",
    "    valid_indices.append(i)\n",
    "    \n",
    "    if len(entry_xyz) > max_len_seen:\n",
    "        max_len_seen = len(entry_xyz)\n",
    "    \n",
    "\n",
    "print(f\"Removed {num_removed_because_nan} sequences because of high NaN ratio.\")\n",
    "print(f\"Removed {num_removed_because_size} sequences because of size.\")\n",
    "print(f\"Removed {num_beginning_removed} sequences because of beginning NaN.\")\n",
    "print(f\"Removed {num_ending_removed} sequences because of ending NaN.\")\n",
    "print(f\"Max length seen: {max_len_seen}\")\n",
    "print(f\"Number of sequences after filtering: {len(valid_indices)}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
