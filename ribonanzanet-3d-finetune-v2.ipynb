{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f90aa192",
   "metadata": {
    "papermill": {
     "duration": 3.737002,
     "end_time": "2025-02-28T10:15:01.952585",
     "exception": false,
     "start_time": "2025-02-28T10:14:58.215583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import general_utils\n",
    "import utils\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "GPUs = GPUtil.getGPUs()\n",
    "if GPUs:\n",
    "    gpu = GPUs[0]\n",
    "    print(f\"Running on: {gpu.name}\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee4e4f",
   "metadata": {
    "papermill": {
     "duration": 0.003294,
     "end_time": "2025-02-28T10:15:01.959744",
     "exception": false,
     "start_time": "2025-02-28T10:15:01.956450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. CONFIG & SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19497539",
   "metadata": {
    "papermill": {
     "duration": 0.014729,
     "end_time": "2025-02-28T10:15:01.977784",
     "exception": false,
     "start_time": "2025-02-28T10:15:01.963055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Set a random seed for Python, NumPy, PyTorch (CPU & GPU) to ensure reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Example configuration (you can load this from a YAML, JSON, etc.)\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"cutoff_date\": \"2020-01-01\",\n",
    "    \"test_cutoff_date\": \"2022-05-01\",\n",
    "    \"max_len\": 384,\n",
    "    \"batch_size\": 1,\n",
    "\n",
    "    # change to kaggle\n",
    "    \"model_config_path\": \"ribonanzanet2d-final/configs/pairwise.yaml\",\n",
    "    \n",
    "    \"max_len_filter\": 9999999,\n",
    "    \"min_len_filter\": 10,\n",
    "    \n",
    "    # change to kaggle\n",
    "    \"ribonanzanet2d-final_path\": \"ribonanzanet2d-final\",\n",
    "    \"train_sequences_path\": \"stanford-rna-3d-folding/train_sequences.csv\",\n",
    "    \"train_labels_path\": \"stanford-rna-3d-folding/train_labels.csv\",\n",
    "    \"test_sequences_path\": \"stanford-rna-3d-folding/test_sequences.csv\",\n",
    "    \"pretrained_weights_path\": \"ribonanzanet-weights/RibonanzaNet.pt\",\n",
    "\n",
    "\n",
    "    \"save_weights_folder\": \"trained_model_weights\",\n",
    "    \"save_weights_name\": \"RibonanzaNet-3D.pt\",\n",
    "    \"save_weights_final\": \"RibonanzaNet-3D-final.pt\",\n",
    "}\n",
    "\n",
    "if not os.path.exists(config['save_weights_folder']):\n",
    "    os.mkdir(config['save_weights_folder'])\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e896b07",
   "metadata": {
    "papermill": {
     "duration": 0.003166,
     "end_time": "2025-02-28T10:15:01.984333",
     "exception": false,
     "start_time": "2025-02-28T10:15:01.981167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. DATA LOADING & PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8b580b",
   "metadata": {
    "papermill": {
     "duration": 8.880616,
     "end_time": "2025-02-28T10:15:10.868369",
     "exception": false,
     "start_time": "2025-02-28T10:15:01.987753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "train_sequences = pd.read_csv(config[\"train_sequences_path\"])\n",
    "train_labels = pd.read_csv(config[\"train_labels_path\"])\n",
    "\n",
    "test_sequences = pd.read_csv(config[\"test_sequences_path\"])\n",
    "\n",
    "# Create a pdb_id field\n",
    "train_labels[\"pdb_id\"] = train_labels[\"ID\"].apply(\n",
    "    lambda x: x.split(\"_\")[0] + \"_\" + x.split(\"_\")[1]\n",
    ")\n",
    "\n",
    "# Collect xyz data for each sequence\n",
    "all_xyz = []\n",
    "for pdb_id in tqdm(train_sequences[\"target_id\"], desc=\"Collecting XYZ data\"):\n",
    "    df = train_labels[train_labels[\"pdb_id\"] == pdb_id]\n",
    "    xyz = df[[\"x_1\", \"y_1\", \"z_1\"]].to_numpy().astype(\"float32\")\n",
    "    xyz[xyz < -1e17] = float(\"nan\")\n",
    "    all_xyz.append(xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e5bea",
   "metadata": {
    "papermill": {
     "duration": 0.006903,
     "end_time": "2025-02-28T10:15:10.883267",
     "exception": false,
     "start_time": "2025-02-28T10:15:10.876364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. DATA FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33f9760d",
   "metadata": {
    "papermill": {
     "duration": 0.025876,
     "end_time": "2025-02-28T10:15:10.916419",
     "exception": false,
     "start_time": "2025-02-28T10:15:10.890543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_indices = []\n",
    "max_len_seen = 0\n",
    "\n",
    "for i, xyz in enumerate(all_xyz):\n",
    "    # Track the maximum length\n",
    "    if len(xyz) > max_len_seen:\n",
    "        max_len_seen = len(xyz)\n",
    "\n",
    "    nan_ratio = np.isnan(xyz).mean()\n",
    "    seq_len = len(xyz)\n",
    "    # Keep sequence if it meets criteria\n",
    "    if (nan_ratio <= 0.5) and (config[\"min_len_filter\"] < seq_len < config[\"max_len_filter\"]):\n",
    "        valid_indices.append(i)\n",
    "\n",
    "print(f\"Longest sequence in train: {max_len_seen}\")\n",
    "\n",
    "# Filter sequences & xyz based on valid_indices\n",
    "train_sequences = train_sequences.loc[valid_indices].reset_index(drop=True)\n",
    "all_xyz = [all_xyz[i] for i in valid_indices]\n",
    "\n",
    "# Prepare final data dictionary\n",
    "data = {\n",
    "    \"sequence\": train_sequences[\"sequence\"].tolist(),\n",
    "    \"temporal_cutoff\": train_sequences[\"temporal_cutoff\"].tolist(),\n",
    "    \"description\": train_sequences[\"description\"].tolist(),\n",
    "    \"all_sequences\": train_sequences[\"all_sequences\"].tolist(),\n",
    "    \"target_id\": train_sequences[\"target_id\"].tolist(),\n",
    "    \"xyz\": all_xyz,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    \"sequence\": test_sequences[\"sequence\"].tolist(),\n",
    "    \"target_id\": test_sequences[\"target_id\"].tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957262a",
   "metadata": {
    "papermill": {
     "duration": 0.006847,
     "end_time": "2025-02-28T10:15:10.931112",
     "exception": false,
     "start_time": "2025-02-28T10:15:10.924265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. TRAIN / VAL SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d6e910a",
   "metadata": {
    "papermill": {
     "duration": 0.015688,
     "end_time": "2025-02-28T10:15:10.954128",
     "exception": false,
     "start_time": "2025-02-28T10:15:10.938440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff_date = pd.Timestamp(config[\"cutoff_date\"])\n",
    "test_cutoff_date = pd.Timestamp(config[\"test_cutoff_date\"])\n",
    "\n",
    "train_indices = [i for i, date_str in enumerate(data[\"temporal_cutoff\"]) if pd.Timestamp(date_str) <= cutoff_date]\n",
    "val_indices = [i for i, date_str in enumerate(data[\"temporal_cutoff\"]) if cutoff_date < pd.Timestamp(date_str) <= test_cutoff_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0c5d",
   "metadata": {
    "papermill": {
     "duration": 0.007268,
     "end_time": "2025-02-28T10:15:10.974877",
     "exception": false,
     "start_time": "2025-02-28T10:15:10.967609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. DATASET & DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1302fc",
   "metadata": {
    "papermill": {
     "duration": 0.015863,
     "end_time": "2025-02-28T10:15:10.997556",
     "exception": false,
     "start_time": "2025-02-28T10:15:10.981693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNA3D_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for 3D RNA structures.\n",
    "    \"\"\"\n",
    "    def __init__(self, indices, data_dict, max_len=384):\n",
    "        self.indices = indices\n",
    "        self.data = data_dict\n",
    "        self.max_len = max_len\n",
    "        self.nt_to_idx = {nt: i for i, nt in enumerate(\"ACGU\")}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_idx = self.indices[idx]\n",
    "        # Convert nucleotides to integer tokens\n",
    "        sequence = [self.nt_to_idx[nt] for nt in self.data[\"sequence\"][data_idx]]\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long)\n",
    "        # Convert xyz to torch tensor\n",
    "        xyz = torch.tensor(self.data[\"xyz\"][data_idx], dtype=torch.float32)\n",
    "\n",
    "        # If sequence is longer than max_len, randomly crop\n",
    "        if len(sequence) > self.max_len:\n",
    "            crop_start = np.random.randint(len(sequence) - self.max_len)\n",
    "            crop_end = crop_start + self.max_len\n",
    "            sequence = sequence[crop_start:crop_end]\n",
    "            xyz = xyz[crop_start:crop_end]\n",
    "\n",
    "        return {\"sequence\": sequence, \"xyz\": xyz, \"target_id\": self.data['target_id'][data_idx]}\n",
    "\n",
    "class RNA3D_Dataset_Test(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for 3D RNA structures.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict, max_len=384):\n",
    "        self.data = data_dict\n",
    "        self.max_len = max_len\n",
    "        self.nt_to_idx = {nt: i for i, nt in enumerate(\"ACGU\")}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"sequence\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert nucleotides to integer tokens\n",
    "        sequence = [self.nt_to_idx[nt] if nt in self.nt_to_idx else 4 for nt in self.data[\"sequence\"][idx]]\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long)\n",
    "        # Convert xyz to torch tensor\n",
    "\n",
    "        # Dont crop and return the full sequence\n",
    "\n",
    "        return {\"sequence\": sequence, \"target_id\": self.data[\"target_id\"][idx]}\n",
    "\n",
    "\n",
    "train_dataset = RNA3D_Dataset(train_indices, data, max_len=config[\"max_len\"])\n",
    "val_dataset = RNA3D_Dataset(val_indices, data, max_len=config[\"max_len\"])\n",
    "test_dataset = RNA3D_Dataset_Test(test_data, max_len=config[\"max_len\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6388efa6",
   "metadata": {
    "papermill": {
     "duration": 0.011346,
     "end_time": "2025-02-28T10:15:11.021895",
     "exception": false,
     "start_time": "2025-02-28T10:15:11.010549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. MODEL, CONFIG CLASSES & HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d295221b",
   "metadata": {
    "papermill": {
     "duration": 4.852189,
     "end_time": "2025-02-28T10:15:15.881166",
     "exception": false,
     "start_time": "2025-02-28T10:15:11.028977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(config[\"ribonanzanet2d-final_path\"])\n",
    "\n",
    "from Network import RibonanzaNet\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Simple Config class that can load from a dict or YAML.\"\"\"\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        self.entries = entries\n",
    "\n",
    "    def print(self):\n",
    "        print(self.entries)\n",
    "\n",
    "def load_config_from_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        cfg = yaml.safe_load(file)\n",
    "    return Config(**cfg)\n",
    "\n",
    "class FinetunedRibonanzaNet(RibonanzaNet):\n",
    "    \"\"\"\n",
    "    A finetuned version of RibonanzaNet adapted for predicting 3D coordinates.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_obj, pretrained=False, dropout=0.1):\n",
    "        # Modify config dropout before super init, if needed\n",
    "        config_obj.dropout = dropout\n",
    "        super(FinetunedRibonanzaNet, self).__init__(config_obj)\n",
    "\n",
    "        # Load pretrained weights if requested\n",
    "        if pretrained:\n",
    "            self.load_state_dict(\n",
    "                torch.load(config[\"pretrained_weights_path\"], map_location=\"cpu\")\n",
    "            )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.xyz_predictor = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"Forward pass to predict 3D XYZ coordinates.\"\"\"\n",
    "        # get_embeddings returns (sequence_features, *some_other_outputs)\n",
    "        sequence_features, _ = self.get_embeddings(\n",
    "            src, torch.ones_like(src).long().to(src.device)\n",
    "        )\n",
    "        xyz_pred = self.xyz_predictor(sequence_features)\n",
    "        return xyz_pred\n",
    "\n",
    "# Instantiate the model\n",
    "model_cfg = load_config_from_yaml(config[\"model_config_path\"])\n",
    "model = FinetunedRibonanzaNet(model_cfg, pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295d511",
   "metadata": {
    "papermill": {
     "duration": 0.007205,
     "end_time": "2025-02-28T10:15:15.895806",
     "exception": false,
     "start_time": "2025-02-28T10:15:15.888601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "343c7934",
   "metadata": {
    "papermill": {
     "duration": 0.019053,
     "end_time": "2025-02-28T10:15:15.921825",
     "exception": false,
     "start_time": "2025-02-28T10:15:15.902772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_distance_matrix(X, Y, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Calculate pairwise distances between every point in X and every point in Y.\n",
    "    Shape: (len(X), len(Y))\n",
    "    \"\"\"\n",
    "    return ((X[:, None] - Y[None, :])**2 + epsilon).sum(dim=-1).sqrt()\n",
    "\n",
    "def dRMSD(pred_x, pred_y, gt_x, gt_y, epsilon=1e-4, Z=10, d_clamp=None):\n",
    "    \"\"\"\n",
    "    Distance-based RMSD.\n",
    "    pred_x, pred_y: predicted coordinates (usually the same tensor for X and Y).\n",
    "    gt_x, gt_y: ground truth coordinates.\n",
    "    \"\"\"\n",
    "    pred_dm = calculate_distance_matrix(pred_x, pred_y)\n",
    "    gt_dm = calculate_distance_matrix(gt_x, gt_y)\n",
    "\n",
    "    mask = ~torch.isnan(gt_dm)\n",
    "    mask[torch.eye(mask.shape[0], device=mask.device).bool()] = False\n",
    "\n",
    "    diff_sq = (pred_dm[mask] - gt_dm[mask])**2 + epsilon\n",
    "    if d_clamp is not None:\n",
    "        diff_sq = diff_sq.clamp(max=d_clamp**2)\n",
    "\n",
    "    return diff_sq.sqrt().mean() / Z\n",
    "\n",
    "def local_dRMSD(pred_x, pred_y, gt_x, gt_y, epsilon=1e-4, Z=10, d_clamp=30):\n",
    "    \"\"\"\n",
    "    Local distance-based RMSD, ignoring distances above a clamp threshold.\n",
    "    \"\"\"\n",
    "    pred_dm = calculate_distance_matrix(pred_x, pred_y)\n",
    "    gt_dm = calculate_distance_matrix(gt_x, gt_y)\n",
    "\n",
    "    mask = (~torch.isnan(gt_dm)) & (gt_dm < d_clamp)\n",
    "    mask[torch.eye(mask.shape[0], device=mask.device).bool()] = False\n",
    "\n",
    "    diff_sq = (pred_dm[mask] - gt_dm[mask])**2 + epsilon\n",
    "    return diff_sq.sqrt().mean() / Z\n",
    "\n",
    "def dRMAE(pred_x, pred_y, gt_x, gt_y, epsilon=1e-4, Z=10):\n",
    "    \"\"\"\n",
    "    Distance-based Mean Absolute Error.\n",
    "    \"\"\"\n",
    "    pred_dm = calculate_distance_matrix(pred_x, pred_y)\n",
    "    gt_dm = calculate_distance_matrix(gt_x, gt_y)\n",
    "\n",
    "    mask = ~torch.isnan(gt_dm)\n",
    "    mask[torch.eye(mask.shape[0], device=mask.device).bool()] = False\n",
    "\n",
    "    diff = torch.abs(pred_dm[mask] - gt_dm[mask])\n",
    "    return diff.mean() / Z\n",
    "\n",
    "def align_svd_mae(input_coords, target_coords, Z=10):\n",
    "    \"\"\"\n",
    "    Align input_coords to target_coords via SVD (Kabsch algorithm) and compute MAE.\n",
    "    \"\"\"\n",
    "    assert input_coords.shape == target_coords.shape, \"Input and target must have the same shape\"\n",
    "\n",
    "    # Create mask for valid points\n",
    "    mask = ~torch.isnan(target_coords.sum(dim=-1))\n",
    "    input_coords = input_coords[mask]\n",
    "    target_coords = target_coords[mask]\n",
    "    \n",
    "    # Compute centroids\n",
    "    centroid_input = input_coords.mean(dim=0, keepdim=True)\n",
    "    centroid_target = target_coords.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Center the points\n",
    "    input_centered = input_coords - centroid_input\n",
    "    target_centered = target_coords - centroid_target\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = input_centered.T @ target_centered\n",
    "\n",
    "    # SVD to find optimal rotation\n",
    "    U, S, Vt = torch.svd(cov_matrix)\n",
    "    R = Vt @ U.T\n",
    "\n",
    "    # Ensure a proper rotation (determinant R == 1)\n",
    "    if torch.det(R) < 0:\n",
    "        Vt_adj = Vt.clone()   # Clone to avoid in-place modification issues\n",
    "        Vt_adj[-1, :] = -Vt_adj[-1, :]\n",
    "        R = Vt_adj @ U.T\n",
    "\n",
    "    # Rotate input and compute mean absolute error\n",
    "    aligned_input = (input_centered @ R.T) + centroid_target\n",
    "    return torch.abs(aligned_input - target_coords).mean() / Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a91daf",
   "metadata": {
    "papermill": {
     "duration": 0.006713,
     "end_time": "2025-02-28T10:15:15.935655",
     "exception": false,
     "start_time": "2025-02-28T10:15:15.928942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b124051",
   "metadata": {
    "papermill": {
     "duration": 0.016374,
     "end_time": "2025-02-28T10:15:15.958920",
     "exception": false,
     "start_time": "2025-02-28T10:15:15.942546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, epochs=50, cos_epoch=35, lr=3e-4, clip=1):\n",
    "    \"\"\"Train the model with a CosineAnnealingLR after `cos_epoch` epochs.\"\"\"\n",
    "    best_model_path = general_utils.get_next_filename(os.path.join(config['save_weights_folder'], config[\"save_weights_name\"]))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.0, lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=(epochs - cos_epoch) * len(train_dl),\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_preds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_dl, desc=f\"Training Epoch {epoch+1}/{epochs}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for idx, batch in enumerate(train_pbar):\n",
    "            sequence = batch[\"sequence\"].cuda()\n",
    "            gt_xyz = batch[\"xyz\"].squeeze().cuda()\n",
    "\n",
    "            pred_xyz = model(sequence).squeeze()\n",
    "\n",
    "            # Combine two distance-based losses\n",
    "            loss = dRMAE(pred_xyz, pred_xyz, gt_xyz, gt_xyz) + align_svd_mae(pred_xyz, gt_xyz)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (epoch + 1) > cos_epoch:\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (idx + 1)\n",
    "            train_pbar.set_description(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(val_dl):\n",
    "                sequence = batch[\"sequence\"].cuda()\n",
    "                gt_xyz = batch[\"xyz\"].squeeze().cuda()\n",
    "\n",
    "                pred_xyz = model(sequence).squeeze()\n",
    "                loss = dRMAE(pred_xyz, pred_xyz, gt_xyz, gt_xyz)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_preds.append((gt_xyz.cpu().numpy(), pred_xyz.cpu().numpy()))\n",
    "\n",
    "            val_loss /= len(val_dl)\n",
    "            print(f\"Validation Loss (Epoch {epoch+1}): {val_loss:.4f}\")\n",
    "\n",
    "            # Check for improvement\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_preds = val_preds\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"  -> New best model saved to {best_model_path} at epoch {epoch+1}\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), general_utils.get_next_filename(os.path.join(config['save_weights_folder'], config[\"save_weights_final\"])))\n",
    "    return best_val_loss, best_preds, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39df71",
   "metadata": {
    "papermill": {
     "duration": 0.006789,
     "end_time": "2025-02-28T10:15:15.972874",
     "exception": false,
     "start_time": "2025-02-28T10:15:15.966085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. RUN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bb38825",
   "metadata": {
    "papermill": {
     "duration": 5803.801452,
     "end_time": "2025-02-28T11:51:59.781364",
     "exception": false,
     "start_time": "2025-02-28T10:15:15.979912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_loss, best_predictions, best_model_path = train_model(\n",
    "    model=model,\n",
    "    train_dl=train_loader,\n",
    "    val_dl=val_loader,\n",
    "    epochs=2,         # or config[\"epochs\"]\n",
    "    cos_epoch=1,      # or config[\"cos_epoch\"]\n",
    "    lr=3e-4,\n",
    "    clip=1\n",
    ")\n",
    "print(f\"Best Validation Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65c49da9-a21e-4839-8bdc-464f25727075",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('EMAIL_PASSWORD') is not None:\n",
    "    general_utils.send_email(os.environ.get('EMAIL_PASSWORD'), \"Training complete\", f\"Model saved to {best_model_path} after {general_utils.get_time_from_start(start_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a83ea",
   "metadata": {},
   "source": [
    "# 10. LOAD MODEL AND EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c063797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_load_path = best_model_path\n",
    "model_load_path = \"trained_model_weights/RibonanzaNet-3D_2.pt\"\n",
    "loaded_model = FinetunedRibonanzaNet(model_cfg, pretrained=True).cuda()\n",
    "loaded_model.load_state_dict(\n",
    "    torch.load(model_load_path, map_location=\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_to_monomer = {i: nt for i, nt in enumerate(\"ACGU\")}\n",
    "def vals_to_monomers(vals):\n",
    "    return [val_to_monomer[i] for i in vals.tolist()]\n",
    "\n",
    "cols = [\"ID\",\"resname\",\"resid\",\"x_1\",\"y_1\",\"z_1\",\"x_2\",\"y_2\",\"z_2\",\"x_3\",\"y_3\",\"z_3\",\"x_4\",\"y_4\",\"z_4\",\"x_5\",\"y_5\",\"z_5\"]\n",
    "preds_pd = pd.DataFrame(columns=cols)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loaded_model.eval()\n",
    "\n",
    "    test_pbar = tqdm(test_loader, desc=f\"Generating Predictions\")\n",
    "\n",
    "    test_preds = []\n",
    "    for idx, batch in enumerate(test_pbar):\n",
    "        sequence = batch[\"sequence\"].cuda()\n",
    "        res_names = vals_to_monomers(sequence.squeeze())\n",
    "        res_ids = [i + 1 for i in range(len(res_names))]\n",
    "        target_id = batch[\"target_id\"]\n",
    "\n",
    "\n",
    "        pred_xyz = loaded_model(sequence).squeeze().cpu().numpy()\n",
    "\n",
    "        new_row = {\n",
    "            \"ID\": [batch['target_id'][0] + \"_\" + str(i) for i in res_ids],\n",
    "            \"resname\": res_names,\n",
    "            \"resid\": res_ids,\n",
    "            \"x_1\": pred_xyz[:, 0],\n",
    "            \"y_1\": pred_xyz[:, 1],\n",
    "            \"z_1\": pred_xyz[:, 2],\n",
    "        }\n",
    "        for i in range(2, 6):\n",
    "            new_row[f\"x_{i}\"] = 0.0\n",
    "            new_row[f\"y_{i}\"] = 0.0\n",
    "            new_row[f\"z_{i}\"] = 0.0\n",
    "        \n",
    "        preds_pd = pd.concat([preds_pd, pd.DataFrame(new_row)], ignore_index=True)\n",
    "\n",
    "preds_pd.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08a9a50e-6693-4689-82ae-d921be71355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    loaded_model.eval()\n",
    "\n",
    "    for idx, this_data in enumerate(val_loader):\n",
    "        if idx == num - 1:\n",
    "            xyz = this_data['xyz'].numpy().squeeze()\n",
    "            sequences = np.array(vals_to_monomers(this_data['sequence'].numpy().squeeze())).astype(object)\n",
    "\n",
    "            \n",
    "            actual = {\n",
    "                \"x\": xyz[:, 0],\n",
    "                \"y\": xyz[:, 1],\n",
    "                \"z\": xyz[:, 2],\n",
    "                \"sequences\": sequences,\n",
    "                \"name\": f\"{this_data['target_id']}-actual\"}\n",
    "        \n",
    "        \n",
    "            pred_sequence = this_data['sequence'].cuda()\n",
    "            pred_xyz = loaded_model(pred_sequence).squeeze().cpu().numpy()\n",
    "\n",
    "        \n",
    "            new_pred_xyz = utils.align_structures(pred_xyz, xyz)\n",
    "            cropped_sequences = sequences[:new_pred_xyz.shape[0]]\n",
    "\n",
    "            # print(pred_xyz.shape)\n",
    "            # print(new_pred_xyz.shape)\n",
    "            # print(sequences.shape)\n",
    "            # print(pred_sequence.shape)\n",
    "\n",
    "\n",
    "        \n",
    "            predicted = {\n",
    "                \"x\": new_pred_xyz[:, 0],\n",
    "                \"y\": new_pred_xyz[:, 1],\n",
    "                \"z\": new_pred_xyz[:, 2],\n",
    "                \"sequences\": cropped_sequences,\n",
    "                \"name\": f\"{this_data['target_id']}-predicted\"}\n",
    "        \n",
    "            \n",
    "            utils.plot_multiple_structures([predicted, actual])\n",
    "            # utils.plot_multiple_structures([actual, predicted])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11228175,
     "sourceId": 87793,
     "sourceType": "competition"
    },
    {
     "datasetId": 4299272,
     "sourceId": 7639698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4459124,
     "sourceId": 8318191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5832.807833,
   "end_time": "2025-02-28T11:52:04.178236",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-28T10:14:51.370403",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
