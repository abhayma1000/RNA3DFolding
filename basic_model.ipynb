{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Develop a model that works. It will be awful, but it works\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('stanford-rna-3d-folding/train_labels.csv')\n",
    "\n",
    "train_sequences = pd.read_csv('stanford-rna-3d-folding/train_sequences.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('stanford-rna-3d-folding/sample_submission.csv')\n",
    "\n",
    "test_sequences = pd.read_csv('stanford-rna-3d-folding/test_sequences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_sequences.head())\n",
    "\n",
    "display(train_labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = train_sequences.drop(['all_sequences', 'temporal_cutoff', 'sequence'], axis=1)\n",
    "\n",
    "train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_with_nan(df: pd.DataFrame):\n",
    "    return df[df.isnull().any(axis=1)]\n",
    "\n",
    "def remove_rows_with_nan(df: pd.DataFrame, exclude_columns=None):\n",
    "    if exclude_columns:\n",
    "        return df.dropna(subset=[col for col in df.columns if col not in exclude_columns])\n",
    "    else:\n",
    "        return df.dropna()\n",
    "\n",
    "def remove_sequences_with_nan(df: pd.DataFrame):\n",
    "    null_df = get_rows_with_nan(df)\n",
    "    null_ids = null_df['ID'].apply(lambda x: x.split('_')[0] + '_' + x.split('_')[1])\n",
    "\n",
    "    return df[~df['ID'].apply(lambda x: x.split('_')[0] + '_' + x.split('_')[1]).isin(null_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = remove_sequences_with_nan(train_labels)\n",
    "train_sequences = remove_rows_with_nan(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data = train_labels.copy()\n",
    "\n",
    "all_train_data['target_id'] = all_train_data['ID'].apply(lambda x: x.split('_')[0] + '_' + x.split('_')[1])\n",
    "\n",
    "all_train_data = pd.merge(all_train_data, train_sequences, left_on='target_id', right_on='target_id', how='left')\n",
    "\n",
    "\n",
    "# now going to take the last two deltas and use that for the prediction\n",
    "# For the labels, going to calculate the delta in each direction\n",
    "\n",
    "coords = ['x_1', 'y_1', 'z_1']\n",
    "\n",
    "for coord in coords:\n",
    "    all_train_data[f'{coord}_delta'] = all_train_data.groupby('target_id')[coord].diff()\n",
    "\n",
    "\n",
    "\n",
    "all_train_data = all_train_data[all_train_data['ID'].apply(lambda x: x.split('_')[2] != '1')]\n",
    "\n",
    "all_train_data = all_train_data.drop(columns=coords)\n",
    "\n",
    "for coord in coords:\n",
    "    all_train_data[f'pred_{coord}_delta'] = 0\n",
    "\n",
    "\n",
    "all_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_name_encoder = OneHotEncoder()\n",
    "res_name_encoder.fit(all_train_data['resname'].values.reshape(-1, 1))\n",
    "\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "max_length = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_sequence(sequence: str, tokenizer: transformers.PreTrainedTokenizer, max_length: int):\n",
    "    encoding = tokenizer(\n",
    "        sequence, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors='pt')\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def get_input_output_tensor(df: pd.DataFrame):\n",
    "    tokenized_data = df['description'].apply(lambda x: tokenize_sequence(x, tokenizer, max_length)).reset_index(drop=True)\n",
    "\n",
    "    input_ids = torch.cat([tokenized_data[i]['input_ids'] for i in range(len(tokenized_data))], dim=0)\n",
    "    attention_mask = torch.cat([tokenized_data[i]['attention_mask'] for i in range(len(tokenized_data))], dim=0)\n",
    "\n",
    "    res_names = torch.tensor(res_name_encoder.transform(df['resname'].values.reshape(-1, 1)).toarray())\n",
    "\n",
    "    pred_delta = torch.tensor(df[[f'pred_{coord}_delta' for coord in coords]].values)\n",
    "\n",
    "\n",
    "    outputs = torch.tensor(df[[f'{coord}_delta' for coord in coords]].values, dtype=torch.float32)\n",
    "\n",
    "    locations = {\n",
    "        'input_ids': (0, input_ids.shape[1]),\n",
    "        'attention_mask': (input_ids.shape[1], input_ids.shape[1] + attention_mask.shape[1]),\n",
    "        'res_names': (input_ids.shape[1] + attention_mask.shape[1], input_ids.shape[1] + attention_mask.shape[1] + res_names.shape[1]),\n",
    "        'pred_delta': (input_ids.shape[1] + attention_mask.shape[1] + res_names.shape[1], input_ids.shape[1] + attention_mask.shape[1] + res_names.shape[1] + pred_delta.shape[1])\n",
    "    }\n",
    "\n",
    "    # print(type(input_ids), type(attention_mask), type(res_names), type(pred_delta), type(outputs))\n",
    "    return torch.cat((input_ids.float(), attention_mask.float(), res_names.float(), pred_delta.float()), dim=1), outputs.float(), locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = all_train_data['target_id'].unique()\n",
    "\n",
    "split_all_train_data = []\n",
    "\n",
    "percentage = 0.5\n",
    "\n",
    "for id in unique_ids[0:int(percentage * len(unique_ids))]:\n",
    "    split_all_train_data.append(all_train_data[all_train_data['target_id'] == id])\n",
    "\n",
    "\n",
    "# split_all_train_data\n",
    "\n",
    "train_inputs = []\n",
    "train_outputs = []\n",
    "\n",
    "\n",
    "\n",
    "for i in split_all_train_data:\n",
    "    input, output, locations = get_input_output_tensor(i)\n",
    "    train_inputs.append(input)\n",
    "    train_outputs.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_inputs)):\n",
    "    print(f\"On #{i}. Input Shape: {train_inputs[i].shape}. Output Shape: {train_outputs[i].shape}\")\n",
    "\n",
    "print(\"Locations:\")\n",
    "display(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for an entire thingy at one time starting at index 2\n",
    "\n",
    "class AutoRegressiveNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AutoRegressiveNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        predictions = []\n",
    "        x_copy = x.detach().clone()\n",
    "\n",
    "        for i in range(len(x_copy)):\n",
    "            fc1_out = self.fc1(x_copy[i])\n",
    "            relu1_out = self.relu1(fc1_out)\n",
    "            fc2_out = self.fc2(relu1_out)\n",
    "            relu2_out = self.relu2(fc2_out)\n",
    "            pred = self.fc3(relu2_out)\n",
    "            predictions.append(pred)\n",
    "\n",
    "            if i < len(x_copy) - 1:\n",
    "                x_copy = x_copy.detach().clone()\n",
    "                x_copy[i+1, -3:] = pred\n",
    "\n",
    "\n",
    "        \n",
    "        return torch.stack(predictions, dim=0)\n",
    "    \n",
    "    def train_model(self, data, target, epochs=100):\n",
    "        losses = []\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "\n",
    "            # print(\"Data shape: \", len(data))\n",
    "            \n",
    "            for i, batch in enumerate(data):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                prediction = self.forward(batch)\n",
    "                # prediction = prediction.detach().clone()\n",
    "                # print(\"Iteration: \", i)\n",
    "                # print(\"Batch shape: \", batch.shape)\n",
    "                # print(\"Output shape: \", target[i].shape)\n",
    "                # print(\"Prediction shape: \", prediction.shape)\n",
    "\n",
    "                loss = criterion(prediction, target[i])\n",
    "\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "            print(f\"Epoch: {epoch}. Loss: {loss.item()}\")\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = AutoRegressiveNN(train_inputs[0].shape[1], 20, train_outputs[0].shape[1])\n",
    "\n",
    "# output_0 = ann.forward(train_inputs[0])\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "losses = ann.train_model(train_inputs, train_outputs, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), np.array(losses))\n",
    "plt.title(\"Losses over time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_output_to_points_for_plotting(input: torch.tensor, output: torch.tensor, target: torch.tensor):\n",
    "    x = [0]\n",
    "    y = [0]\n",
    "    z = [0]\n",
    "\n",
    "    actual_x = [0]\n",
    "    actual_y = [0]\n",
    "    actual_z = [0]\n",
    "\n",
    "    sequences = ['G'] # TODO this is wrong. Somehow keep the previous sequence in the training data and put here\n",
    "\n",
    "    for i in range(len(input)):\n",
    "        res_input = input[i, locations['res_names'][0]:locations['res_names'][1]]\n",
    "        transformed_res_input = res_name_encoder.inverse_transform(res_input.detach().numpy().reshape(1, -1))\n",
    "        sequences.append(transformed_res_input[0][0])\n",
    "\n",
    "\n",
    "\n",
    "        x.append(x[-1] + output[i, 0].detach().item())\n",
    "        y.append(y[-1] + output[i, 1].detach().item())\n",
    "        z.append(z[-1] + output[i, 2].detach().item())\n",
    "\n",
    "\n",
    "        actual_x.append(actual_x[-1] + target[i, 0].detach().item())\n",
    "        actual_y.append(actual_y[-1] + target[i, 1].detach().item())\n",
    "        actual_z.append(actual_z[-1] + target[i, 2].detach().item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(x), np.array(y), np.array(z), np.array(actual_x), np.array(actual_y), np.array(actual_z), np.array(sequences).astype(object)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_0 = ann.forward(train_inputs[0])\n",
    "\n",
    "x, y, z, actual_x, actual_y, actual_z, sequences = convert_output_to_points_for_plotting(train_inputs[0], output_0, train_outputs[0])\n",
    "name = 'idk something'\n",
    "combined = [[x, y, z, sequences, name], [actual_x, actual_y, actual_z, sequences, name]]\n",
    "\n",
    "plot_multiple_structures(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
